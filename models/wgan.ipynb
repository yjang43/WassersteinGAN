{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd3a0f68-199c-4ec9-b68b-b2d1f7571744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname == 'ConvTranspose2d' or classname == 'Conv2d':\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname == 'BatchNorm2d':\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, conv_kwargs, activation='leaky_relu', normalization='batch_normalization'):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(**conv_kwargs)\n",
    "        \n",
    "        if normalization == 'batch_normalization': self.norm = nn.BatchNorm2d(conv_kwargs['out_channels'])\n",
    "        elif normalization == 'instance_normalization': self.norm = nn.InstanceNorm2d(conv_kwargs['out_channels'])\n",
    "        elif normalization is None: self.norm = nn.Sequential()\n",
    "        \n",
    "        if activation == 'leaky_relu': self.actv = nn.LeakyReLU(0.2)\n",
    "        elif activation is None: self.actv = nn.Sequential()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.actv(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class WGANDiscriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(WGANDiscriminator, self).__init__()\n",
    "        depth = int(math.log2(args.resolution)) -1\n",
    "\n",
    "        net = [ConvBlock(\n",
    "            {\n",
    "                'in_channels': args.nc,\n",
    "                'out_channels': args.ngf,\n",
    "                'kernel_size': 4,\n",
    "                'stride': 2,\n",
    "                'padding': 1\n",
    "            },\n",
    "            normalization=None\n",
    "        )]\n",
    "        \n",
    "        mult = 1\n",
    "        for _ in range(depth - 2):\n",
    "            net.append(ConvBlock(\n",
    "                {\n",
    "                    'in_channels': args.ngf * mult,\n",
    "                    'out_channels': args.ngf * mult * 2,\n",
    "                    'kernel_size': 4,\n",
    "                    'stride': 2,\n",
    "                    'padding': 1\n",
    "                }\n",
    "            ))\n",
    "            mult *= 2\n",
    "        net.append(ConvBlock(\n",
    "            {\n",
    "                'in_channels': args.ngf * mult,\n",
    "                'out_channels': 1,\n",
    "                'kernel_size': 4,\n",
    "                'stride': 1,\n",
    "                'padding': 0\n",
    "            },\n",
    "            normalization=None\n",
    "        ))\n",
    "        self.net = nn.Sequential(*net)\n",
    "        self.apply(weights_init)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "\n",
    "class ConvTransposeBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, deconv_kwargs, activation='relu', normalization='batch_normalization'):\n",
    "        super(ConvTransposeBlock, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(**deconv_kwargs)\n",
    "        \n",
    "        if normalization == 'batch_normalization': self.norm = nn.BatchNorm2d(deconv_kwargs['out_channels'])\n",
    "        elif normalization == 'instance_normalization': self.norm = nn.InstanceNorm2d(deconv_kwargs['out_channels'])\n",
    "        elif normalization is None: self.norm = nn.Sequential()\n",
    "        \n",
    "        if activation == 'relu': self.actv = nn.ReLU()\n",
    "        elif activation == 'tanh': self.actv = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.deconv(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.actv(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class WGANGenerator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(WGANGenerator, self).__init__()\n",
    "        depth = int(math.log2(args.resolution)) -1\n",
    "        \n",
    "        mult = 2 ** (depth - 2)\n",
    "        net = [ConvTransposeBlock(\n",
    "            {\n",
    "                'in_channels': args.nz,\n",
    "                'out_channels': args.ngf * mult,\n",
    "                'kernel_size': 4,\n",
    "                'stride': 1,\n",
    "                'padding': 0\n",
    "            },\n",
    "        )]\n",
    "        for _ in range(depth - 2):\n",
    "            mult = int(mult * 0.5)\n",
    "            net.append(ConvTransposeBlock(\n",
    "                {\n",
    "                    'in_channels': args.ngf * mult * 2,\n",
    "                    'out_channels': args.ngf * mult,\n",
    "                    'kernel_size': 4,\n",
    "                    'stride': 2,\n",
    "                    'padding': 1\n",
    "                },\n",
    "            ))\n",
    "        net.append(ConvTransposeBlock(\n",
    "            {\n",
    "                'in_channels': args.ngf * mult,\n",
    "                'out_channels': args.nc,\n",
    "                'kernel_size': 4,\n",
    "                'stride': 2,\n",
    "                'padding': 1\n",
    "            },\n",
    "            activation='tanh',\n",
    "            normalization=None\n",
    "        ))\n",
    "        \n",
    "        self.net = nn.Sequential(*net)\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class WGANLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(WGANLoss, self).__init__()\n",
    "        self.device = args.device\n",
    "        \n",
    "    def forward(self, x, mode='discriminator_loss'):\n",
    "        if mode == 'discriminator_loss':\n",
    "            fake_pred, real_pred = x\n",
    "            # maximize Wasserstein Distance\n",
    "            loss = -real_pred.mean(0) + fake_pred.mean(0)\n",
    "            \n",
    "        elif mode == 'generator_loss':\n",
    "            fake_pred, _ = x\n",
    "            # minimize Wasserstein Distance\n",
    "            loss = -fake_pred.mean(0)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class WGANOptimizer(optim.RMSprop):\n",
    "    def __init__(self, args, params):\n",
    "        self.args = args\n",
    "        self.params = params\n",
    "        \n",
    "        super(WGANOptimizer, self).__init__(\n",
    "            params,\n",
    "            lr=args.lr,\n",
    "        )\n",
    "\n",
    "    def step(self):\n",
    "        loss = super(, self).step()\n",
    "        # clipping weights\n",
    "        for p in self.params:\n",
    "            p.data.clamp_(args)\n",
    "        return loss\n",
    "\n",
    "components = {\n",
    "    'generator': WGANGenerator,\n",
    "    'discriminator': WGANDiscriminator,\n",
    "    'criterion': WGANLoss,\n",
    "    'optimizer': WGANOptimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c347fd-0974-4a9a-b841-5cebd735ec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[0.3753]]]], requires_grad=True)\n",
      "tensor([[[[0.3753]]]])\n",
      "tensor([[[[0.0010]]]])\n",
      "Parameter containing:\n",
      "tensor([-0.8402], requires_grad=True)\n",
      "tensor([-0.8402])\n",
      "tensor([-0.0010])\n"
     ]
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(1 ,1 ,1 ,1)\n",
    "for p in conv.parameters():\n",
    "    print(p)\n",
    "    print(p.data)\n",
    "    print(p.data.clamp_(-.001, .001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eeb067e-336c-4a5a-832b-146d1d186fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3705])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.randn((10, 1)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974c611-7a2d-4408-a5b4-ebdd7130445e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
